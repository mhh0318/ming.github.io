<!DOCTYPE html>
<html lang="en">
    <!-- title -->




<!-- keywords -->




<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="author" content="M&">
    <meta name="renderer" content="webkit">
    <meta name="copyright" content="M&">
    
    <meta name="keywords" content="明|MING's Studio,M&">
    
    <meta name="description" content>
    <meta http-equiv="Cache-control" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Optimizers · 明 | Ming&#39;s Studio</title>
    <style type="text/css">
    @font-face {
        font-family: 'Oswald-Regular';
        src: url("/font/Oswald-Regular.ttf");
    }

    body {
        margin: 0;
    }

    header,
    footer,
    .back-top,
    .sidebar,
    .container,
    .site-intro-meta,
    .toc-wrapper {
        display: none;
    }

    .site-intro {
        position: relative;
        z-index: 3;
        width: 100%;
        /* height: 50vh; */
        overflow: hidden;
    }

    .site-intro-placeholder {
        position: absolute;
        z-index: -2;
        top: 0;
        left: 0;
        width: calc(100% + 300px);
        height: 100%;
        background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
        background-position: center center;
        transform: translate3d(-226px, 0, 0);
        animation: gradient-move 2.5s ease-out 0s infinite;
    }

    @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }

</style>

    <link rel="preload" href="/css/style.css?v=20180824" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link rel="stylesheet" href="/css/mobile.css?v=20180824" media="(max-width: 980px)">
    
    <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    
    <!-- /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/ -->
<script>
(function( w ){
	"use strict";
	// rel=preload support test
	if( !w.loadCSS ){
		w.loadCSS = function(){};
	}
	// define on the loadCSS obj
	var rp = loadCSS.relpreload = {};
	// rel=preload feature support test
	// runs once and returns a function for compat purposes
	rp.support = (function(){
		var ret;
		try {
			ret = w.document.createElement( "link" ).relList.supports( "preload" );
		} catch (e) {
			ret = false;
		}
		return function(){
			return ret;
		};
	})();

	// if preload isn't supported, get an asynchronous load by using a non-matching media attribute
	// then change that media back to its intended value on load
	rp.bindMediaToggle = function( link ){
		// remember existing media attr for ultimate state, or default to 'all'
		var finalMedia = link.media || "all";

		function enableStylesheet(){
			link.media = finalMedia;
		}

		// bind load handlers to enable media
		if( link.addEventListener ){
			link.addEventListener( "load", enableStylesheet );
		} else if( link.attachEvent ){
			link.attachEvent( "onload", enableStylesheet );
		}

		// Set rel and non-applicable media type to start an async request
		// note: timeout allows this to happen async to let rendering continue in IE
		setTimeout(function(){
			link.rel = "stylesheet";
			link.media = "only x";
		});
		// also enable media after 3 seconds,
		// which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
		setTimeout( enableStylesheet, 3000 );
	};

	// loop through link elements in DOM
	rp.poly = function(){
		// double check this to prevent external calls from running
		if( rp.support() ){
			return;
		}
		var links = w.document.getElementsByTagName( "link" );
		for( var i = 0; i < links.length; i++ ){
			var link = links[ i ];
			// qualify links to those with rel=preload and as=style attrs
			if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
				// prevent rerunning on link
				link.setAttribute( "data-loadcss", true );
				// bind listeners to toggle media back
				rp.bindMediaToggle( link );
			}
		}
	};

	// if unsupported, run the polyfill
	if( !rp.support() ){
		// run once at least
		rp.poly();

		// rerun poly on an interval until onload
		var run = w.setInterval( rp.poly, 500 );
		if( w.addEventListener ){
			w.addEventListener( "load", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		} else if( w.attachEvent ){
			w.attachEvent( "onload", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		}
	}


	// commonjs
	if( typeof exports !== "undefined" ){
		exports.loadCSS = loadCSS;
	}
	else {
		w.loadCSS = loadCSS;
	}
}( typeof global !== "undefined" ? global : this ) );
</script>

    <link rel="icon" href="/assets/favicon.ico">
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js" as="script">
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" as="script">
    <link rel="preload" href="/scripts/main.js" as="script">
    <link rel="preload" as="font" href="/font/Oswald-Regular.ttf" crossorigin>
    <link rel="preload" as="font" href="https://at.alicdn.com/t/font_327081_1dta1rlogw17zaor.woff" crossorigin>
    
    <!-- fancybox -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script>
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  -->
    
</head>

    
        <body class="post-body">
    
    
<header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/"> HomePage </a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">Optimizers</a>
            </div>
    </div>
    
    <a class="home-link" href="/"> Home Page </a>
</header>
    <div class="wrapper">
        <div class="site-intro" style="







height:50vh;
">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-img" style="background-image: url(https://source.unsplash.com/2560x863)"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            Optimizers
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <div class="post-intros">
                <!-- 文章页标签  -->
                
                    <div class="post-intro-tags">
    
        <a class="post-tag" href="javascript:void(0);" data-tags="mini-Batch">mini-Batch</a>
    
        <a class="post-tag" href="javascript:void(0);" data-tags="SGD">SGD</a>
    
        <a class="post-tag" href="javascript:void(0);" data-tags="Adam">Adam</a>
    
        <a class="post-tag" href="javascript:void(0);" data-tags="Adagrad">Adagrad</a>
    
        <a class="post-tag" href="javascript:void(0);" data-tags="Momentum">Momentum</a>
    
</div>
                
                
                    <div class="post-intro-read">
                        <span>Word count: <span class="post-count word-count">1.7k</span>Reading time: <span class="post-count reading-time">10 min</span></span>
                    </div>
                
                <div class="post-intro-meta">
                    <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                    <span class="post-intro-time">2019/04/12</span>
                    
                    <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                        <span class="iconfont-archer">&#xe602;</span>
                        <span id="busuanzi_value_page_pv"></span>
                    </span>
                    
                    <span class="shareWrapper">
                        <span class="iconfont-archer shareIcon">&#xe71d;</span>
                        <span class="shareText">Share</span>
                        <ul class="shareList">
                            <li class="iconfont-archer share-qr" data-type="qr">&#xe75b;
                                <div class="share-qrcode"></div>
                            </li>
                            <li class="iconfont-archer" data-type="weibo">&#xe619;</li>
                            <li class="iconfont-archer" data-type="qzone">&#xe62e;</li>
                            <li class="iconfont-archer" data-type="twitter">&#xe634;</li>
                            <li class="iconfont-archer" data-type="facebook">&#xe67a;</li>
                        </ul>
                    </span>
                </div>
            </div>
        
    </div>
</div>
        <script>
 
  // get user agent
  var browser = {
    versions: function () {
      var u = window.navigator.userAgent;
      return {
        userAgent: u,
        trident: u.indexOf('Trident') > -1, //IE内核
        presto: u.indexOf('Presto') > -1, //opera内核
        webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
        gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
        mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
        ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
        android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
        iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
        iPad: u.indexOf('iPad') > -1, //是否为iPad
        webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
        weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
        uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
      };
    }()
  }
  console.log("userAgent:" + browser.versions.userAgent);

  // callback
  function fontLoaded() {
    console.log('font loaded');
    if (document.getElementsByClassName('site-intro-meta')) {
      document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
      document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
      var postIntros = document.getElementsByClassName('post-intros')[0]
      if (postIntros) {
        postIntros.classList.add('post-fade-in');
      }
    }
  }

  // UC不支持跨域，所以直接显示
  function asyncCb(){
    if (browser.versions.uc) {
      console.log("UCBrowser");
      fontLoaded();
    } else {
      WebFont.load({
        custom: {
          families: ['Oswald-Regular']
        },
        loading: function () {  //所有字体开始加载
          // console.log('loading');
        },
        active: function () {  //所有字体已渲染
          fontLoaded();
        },
        inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
          console.log('inactive: timeout');
          fontLoaded();
        },
        timeout: 5000 // Set the timeout to two seconds
      });
    }
  }

  function asyncErr(){
    console.warn('script load from CDN failed, will load local script')
  }

  // load webfont-loader async, and add callback function
  function async(u, cb, err) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (cb) { o.addEventListener('load', function (e) { cb(null, e); }, false); }
    if (err) { o.addEventListener('error', function (e) { err(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }

  var asyncLoadWithFallBack = function(arr, success, reject) {
      var currReject = function(){
        reject()
        arr.shift()
        if(arr.length)
          async(arr[0], success, currReject)
        }

      async(arr[0], success, currReject)
  }

  asyncLoadWithFallBack([
    "https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js", 
    "https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js",
    "/lib/webfontloader.min.js"
  ], asyncCb, asyncErr)
</script>        
        <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <p>In iteration process, it is expensive to compute the smallest value of cost function by gradient descent. The optimizers are some optimization algorithms of gradient descent. </p>
<h1 id="Vanilla-Gradient-Descent"><a href="#Vanilla-Gradient-Descent" class="headerlink" title="Vanilla Gradient Descent"></a>Vanilla Gradient Descent</h1><h2 id="Batch-Gradient-Descent"><a href="#Batch-Gradient-Descent" class="headerlink" title="Batch Gradient Descent"></a>Batch Gradient Descent</h2><p>In my previous post <a href="http://mhugh.top/2019/03/25/Summary-1/" target="_blank" rel="noopener">Shallow Neural Network</a>, after we find the cost function, the method we use to update the parameters and get the cost is batch gradient descent. It computes the gradient with the entire dataset, it can be expressed as $$ \theta = \theta  - \alpha \frac{dJ(\theta)}{d\theta}$$ </p>
<p>This method will gradually converge to global minimum for a convex error function or to a local minimum for non-convex error function. It is stable, but too slow, especially to a big dataset.</p>
<h2 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h2><p>If BGD is too slow, how about change the batch size? Firstly we can try to set the batch size to a extreme condition as $1$, and the Batch Gradient Descent becomes Stochastic Gradient Descent: $$ \theta = \theta - \alpha \frac{dJ(\theta^{[i]})}{d\theta^{[i]}} $$</p>
<p>The method becomes fast, but it has a high virance. SGD is fluctuation, as the pictures shows</p>
<p><img src="/images/3-SGD.png" alt></p>
<p>It will “jump” to the minimum, and keep oscillating around the minimum point. Here is an illustration of BGD adn SGD:</p>
<p><img src="/images/3-sgd_gd.png" alt></p>
<h2 id="mini-Batch-Gradient-Descent"><a href="#mini-Batch-Gradient-Descent" class="headerlink" title="mini-Batch Gradient Descent"></a>mini-Batch Gradient Descent</h2><p>If we set the batch size to a intermediate number of example, it becomes mini-Batch Gradient Descent: $$ \theta = \theta - \alpha \frac{dJ(\theta^{[i:i+n]})}{d\theta^{[i:i+n]}} $$ where $n$ is the batch size.</p>
<p>This way can reduce the variance of parameter updates, which can make the converge more stable. And it also keep the Pros of SGD - speed. Here, $n$, the batch size is a hyperparameter, common sizes are the power of 2 and in range $[64,256]$  </p>
<p><img src="/images/3-sgd_bgd.png" alt></p>
<h2 id="Summary-of-Vanilla-Gradient-Descent"><a href="#Summary-of-Vanilla-Gradient-Descent" class="headerlink" title="Summary of Vanilla Gradient Descent"></a>Summary of Vanilla Gradient Descent</h2><table>
<thead>
<tr>
<th><strong>Method</strong></th>
<th><em>Accuracy</em></th>
<th><em>Update Speed</em></th>
<th><em>Memory Usage</em></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Batch</strong> gradient Descent</td>
<td>Good</td>
<td>Slow</td>
<td>High</td>
</tr>
<tr>
<td><strong>Stochastic</strong> gradient Descent</td>
<td>Good  “with annealing”</td>
<td>High</td>
<td>Low</td>
</tr>
<tr>
<td><strong>mini-Batch</strong> gradient Descent</td>
<td>Good</td>
<td>Medium</td>
<td>Medium</td>
</tr>
</tbody>
</table>
<h1 id="Optimization-Algorithms"><a href="#Optimization-Algorithms" class="headerlink" title="Optimization Algorithms"></a>Optimization Algorithms</h1><h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations. Momentum takes into account the past gradients to smooth out the update. We will store the ‘direction’ of the previous gradients in the variable $v$. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of $v$ as the “velocity” of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill.<br>$$ v_{d\theta} = \beta v_{d(\theta-1)} + (1-\beta) \frac{dJ(\theta)}{\theta} $$$$ \theta = \theta - \alpha v_{d\theta} $$<br>where $\alpha$ is the learning rate and $\beta$ is another hyperparameter, which is usually set to $0.9$</p>
<p><img src="/images/3-Momentum.png" alt></p>
<p>The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.</p>
<h2 id="Nesterov-accelerated-gradient"><a href="#Nesterov-accelerated-gradient" class="headerlink" title="Nesterov accelerated gradient"></a>Nesterov accelerated gradient</h2><p>We’d like to have a smarter $\theta$ which has a notion where it is going so it will decrease when deviation from the target. </p>
<p>$$ v_{d\theta} = \beta v_{d(\theta-1)} + (1-\beta) \frac{dJ(\theta - \beta v_{d(\theta-1)})}{\theta} $$$$ \theta = \theta - \alpha v_{d\theta} $$</p>
<p>It looks like the $\theta$ can predict the gradient of next time and tune according to the goal. Here is an image can explain the the process of NAG:</p>
<p><img src="/images/3-nag.png" alt></p>
<p>While Momentum first computes the current gradient (small blue vector) and then takes a big jump in the direction of the updated accumulated gradient (big blue vector), NAG first makes a big jump in the direction of the previous accumulated gradient (brown vector), measures the gradient and then makes a correction (red vector), which results in the complete NAG update (green vector). This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of RNNs on a number of tasks</p>
<p>And here is another explaination about NAG <a href="https://zhuanlan.zhihu.com/p/22810533" target="_blank" rel="noopener">比Momentum更快：揭开Nesterov Accelerated Gradient的真面目</a></p>
<h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><p>Adagrad is an algorithm that adapts the learning rate to the parameters. For those high frequency parameters, decrease the learning rate, and for those infrequent parameters, increase the learning rate. Previously, we performed an update for all parameters $\theta$ at once as every parameter $\theta_i$ used the same learning rate $\alpha$. </p>
<p>Adagrad modify the learning rate $\alpha$ based on the past gradients that have been computed. $$ \theta = \theta - \frac{\alpha}{\sqrt{G_t+\epsilon}} \frac{dJ(\theta)}{d\theta} $$ where $G_t$ is a diagonal matrix where each diagonal element is the sum of the squares of the past gradients, while $\epsilon$ is a smoothing term avoiding the denominator is $0$, usually set to $1e-8$. </p>
<p>Adagrad is well-suited for dealing with sparse data and improve the robustness of SGD. But the learning rate will be infinitesimally small after enough itertions.</p>
<h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p>RMSProp modifies the gradient accumulation of AdaGrad to an exponentially weighted moving average, making it work better under a non-convex setting.$$ E[d\theta^2] = {\beta} E[d\theta^2] + (1-\beta) (\frac{dJ(\theta)}{d\theta})^2 $$$$ \theta = \theta - \frac{\alpha}{\sqrt{E[d\theta^2]+\epsilon}}\frac{dJ(\theta)}{d\theta} $$</p>
<h2 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h2><p>Adadelta overcomes the major drawback in Adagrad: monotone decreasing learning rate due to the increasing $G_t$. It restricts the window of accumulated past gradients.</p>
<p>In RMSProp $$\Delta \theta = -\frac{\alpha}{\sqrt{E[d\theta^2]+\epsilon}} \frac{dJ(\theta)}{d\theta}$$ $$E[d\theta^2] = {\beta} E[d\theta^2] + (1-\beta) (\frac{dJ(\theta)}{d\theta})^2$$The authors note that the units in this update (as well as in SGD, Momentum, or Adagrad) do not match, i.e. the update should have the same hypothetical units as the parameter. To realize this, they first define another exponentially decaying average, this time not of squared gradients but of squared parameter updates: $$E[\Delta \theta^2] = {\beta} E[\Delta \theta^2] + (1-\beta) (-\frac{\alpha}{\sqrt{E[d\theta^2]+\epsilon}} \frac{dJ(\theta)}{d\theta})^2$$. Then we replace the learning rate $\alpha$ by $E[\Delta \theta^2]$ in RMSProp, we can get the Adadelta update rule:$$\Delta \theta = - \frac{\sqrt{E[\Delta \theta^2]+\epsilon}}{\sqrt{E[d\theta^2]+\epsilon}} \frac{dJ(\theta)}{d\theta}$$ $$\theta = \theta + \Delta \theta$$</p>
<h2 id="Adaptive-Moment-Estimation"><a href="#Adaptive-Moment-Estimation" class="headerlink" title="Adaptive Moment Estimation"></a>Adaptive Moment Estimation</h2><p>Adam is one of the most commonly used optimization algorithm. As the name shows, it contains Momentum, storing running average of past gradients, and Adaprop, which stores running average of past squared gradients. </p>
<p>First Moment $$ v = \beta_1 v + (1-\beta_1)\frac{dJ(\theta)}{d\theta} $$<br>Second Moment $$ s = \beta_2 s + (1-\beta_2)(\frac{dJ(\theta)}{d\theta})^2$$</p>
<p>All of the parameters are intiallized as $0$. Thus at the first several iteration, $v$ and $s$ are bias to $0$, especially when $\beta_1$ and $\beta_2$ are close to $1$. So after bias correction, we can get $$ v^{corrected} = \frac{v}{1-{\beta_1}^T}$$ $$s^{corrected} = \frac{s}{1-{\beta_2}^T}$$</p>
<p>They then use these to update the parameters just as we have seen in Momentum and RMSprop, which yields the Adam update rule: $$\theta = \theta - \alpha \frac{ v^{corrected}}{\sqrt{s^{corrected}}+\epsilon}$$</p>
<h2 id="AMSGrad"><a href="#AMSGrad" class="headerlink" title="AMSGrad"></a>AMSGrad</h2><blockquote><p>Following AMSGrad content is referring to </p>
<footer><strong>Sebastian Ruder</strong><cite><a href="http://ruder.io/optimizing-gradient-descent/index.html#fnref19" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a></cite></footer></blockquote>
<p>As adaptive learning rate methods have become the norm in training neural networks, practitioners noticed that in some cases, e.g. for object recognition or machine translation they fail to converge to an optimal solution and are outperformed by SGD with momentum.</p>
<p>Reddi et al. (2018) formalize this issue and pinpoint the exponential moving average of past squared gradients as a reason for the poor generalization behaviour of adaptive learning rate methods. Recall that the introduction of the exponential average was well-motivated: It should prevent the learning rates to become infinitesimally small as training progresses, the key flaw of the Adagrad algorithm. However, this short-term memory of the gradients becomes an obstacle in other scenarios.</p>
<p>In settings where Adam converges to a suboptimal solution, it has been observed that some minibatches provide large and informative gradients, but as these minibatches only occur rarely, exponential averaging diminishes their influence, which leads to poor convergence. The authors provide an example for a simple convex optimization problem where the same behaviour can be observed for Adam.</p>
<p>To fix this behaviour, the authors propose a new algorithm, AMSGrad that uses the maximum of past squared gradients $s_t$ rather than the exponential average to update the parameters. $s_t$ is defined the same as in Adam above:$$s_t = \beta_2 s_{t-1} + (1-\beta_2) (\frac{dJ(\theta)}{d\theta})^2 $$ Instead of using<br>$s_t$(or its bias-corrected version) directly, we now employ the previous $s_{t-1}$ if it is larger than the current one: $$\hat{s_t} = max(\hat{s}_{t-1},s_t)$$ $v$ is the same as Adam, so the AMSGrad without bias-corrected can be seen below:$$\theta = \theta - \alpha \frac{v}{\sqrt{\hat{s_t}}+\epsilon}$$</p>
<h1 id="Some-Conclusions"><a href="#Some-Conclusions" class="headerlink" title="Some Conclusions"></a>Some Conclusions</h1><h2 id="Update-Equations"><a href="#Update-Equations" class="headerlink" title="Update Equations"></a>Update Equations</h2><table>
<thead>
<tr>
<th><strong>Method</strong></th>
<th><em>Update Equation</em></th>
</tr>
</thead>
<tbody>
<tr>
<td>SGD</td>
<td>$$\Delta \theta_t = - \alpha g_t$$</td>
</tr>
<tr>
<td>Momentum</td>
<td>$$\Delta \theta_t = -\beta v_{t-1} - \alpha g_t$$</td>
</tr>
<tr>
<td>NAG</td>
<td>$$\Delta \theta_t = -\beta v_{t-1} - \alpha \nabla J(\theta - \alpha v_{t-1})$$</td>
</tr>
<tr>
<td>Adagrad</td>
<td>$$\Delta \theta_t = - \frac{\alpha}{\sqrt{G_t+\epsilon}} g_t$$</td>
</tr>
<tr>
<td>RMSProp</td>
<td>$$\Delta \theta_t = - \frac{\alpha}{\sqrt{E[g^2]+\epsilon}} g_t$$</td>
</tr>
<tr>
<td>Adadelta</td>
<td>$$\Delta \theta = - \frac{\sqrt{E[\Delta \theta^2]+\epsilon}}{\sqrt{E[d\theta^2]+\epsilon}} \frac{dJ(\theta)}{d\theta}$$</td>
</tr>
<tr>
<td>Adam</td>
<td>$$\Delta \theta = - \alpha \frac{ v^{corrected}}{\sqrt{s^{corrected}}+\epsilon}$$</td>
</tr>
<tr>
<td>AMSGrad</td>
<td>$$\Delta \theta = - \alpha \frac{v}{\sqrt{\hat{s_t}}+\epsilon}$$</td>
</tr>
</tbody>
</table>
<p>where $g_t = \nabla J(\theta_t)$ and $\theta_t = \theta_t + \Delta \theta_t$</p>
<h2 id="Visualization-of-Algorithms"><a href="#Visualization-of-Algorithms" class="headerlink" title="Visualization of Algorithms"></a>Visualization of Algorithms</h2><blockquote><p>The visualization of algorithms refers to </p>
<footer><strong>Louis Tiao</strong><cite><a href="http://louistiao.me/notes/visualizing-and-animating-optimization-algorithms-with-matplotlib/" target="_blank" rel="noopener">Visualizing and Animating Optimization Algorithms with Matplotlib</a></cite></footer></blockquote>
<table><tr><td><a><img src="/images/3-opt1.gif"></a></td><td><a><img src="/images/3-opt2.gif"></a></td></tr></table>

<h2 id="Which-algorithm-to-use"><a href="#Which-algorithm-to-use" class="headerlink" title="Which algorithm to use"></a>Which algorithm to use</h2><ul>
<li>Adaptive Learning rate methods (AdaGrad/Adadelta/RMSProp/Adam) are particularly useful for sparse features</li>
<li>Adam might be the best overall choice.</li>
<li>Many recent papers use vanilla SGD without momentum and a simple learning rate annealing schedule. This post give an insight <a href="https://zhuanlan.zhihu.com/p/32230623" target="_blank" rel="noopener">Adam那么棒，为什么还对SGD念念不忘</a></li>
</ul>

    </article>
    <!-- license  -->
    
    <!-- paginator  -->
    <ul class="post-paginator">
        <li class="next">
            
                <div class="nextSlogan">Next Post</div>
                <a href="/2019/04/14/K-Means/" title="K-Means Cluster | Python Implement">
                    <div class="nextTitle">K-Means Cluster | Python Implement</div>
                </a>
            
        </li>
        <li class="previous">
            
                <div class="prevSlogan">Previous Post</div>
                <a href="/2019/04/12/ResNet&DenseNet/" title="ResNet & DenseNet">
                    <div class="prevTitle">ResNet & DenseNet</div>
                </a>
            
        </li>
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    
    <!-- partial('_partial/comment/changyan') -->
    <!--PC版-->


    
    
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
    <div id="comment"></div>
    <script>
    new Valine({
        el: '#comment' ,
        notify:false, 
        verify:false, 
        appId: "NSjyzLH1Tpagwt9WJy9mTjak-gzGzoHsz",
        appKey: "ud7gLUDDmxtfrI0rUEboicQe",
        placeholder: "Do not be shy to chat here, guys.",
        path:window.location.pathname, 
        avatar:'mm' 
    });
    </script>


    <!-- 评论 -->
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:mercihu0318@gmail.com" class="iconfont-archer email" title="email"></a>
            
        
    
        
            
                <a href="//github.com/mhh0318" class="iconfont-archer github" target="_blank" title="github"></a>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">A Rookie in <a target="_blank"> Machine Learning </a>
    </span></div>
    <!-- 不蒜子  -->
    
    <div class="busuanzi-container">
    
     
    <span id="busuanzi_container_site_pv">PV : <span id="busuanzi_value_site_pv"></span></span>
    
    </div>
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper" style=
    







top:50vh;

    >
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Vanilla-Gradient-Descent"><span class="toc-number">1.</span> <span class="toc-text">Vanilla Gradient Descent</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Batch-Gradient-Descent"><span class="toc-number">1.1.</span> <span class="toc-text">Batch Gradient Descent</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Stochastic-Gradient-Descent"><span class="toc-number">1.2.</span> <span class="toc-text">Stochastic Gradient Descent</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#mini-Batch-Gradient-Descent"><span class="toc-number">1.3.</span> <span class="toc-text">mini-Batch Gradient Descent</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Summary-of-Vanilla-Gradient-Descent"><span class="toc-number">1.4.</span> <span class="toc-text">Summary of Vanilla Gradient Descent</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Optimization-Algorithms"><span class="toc-number">2.</span> <span class="toc-text">Optimization Algorithms</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Momentum"><span class="toc-number">2.1.</span> <span class="toc-text">Momentum</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Nesterov-accelerated-gradient"><span class="toc-number">2.2.</span> <span class="toc-text">Nesterov accelerated gradient</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adagrad"><span class="toc-number">2.3.</span> <span class="toc-text">Adagrad</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RMSProp"><span class="toc-number">2.4.</span> <span class="toc-text">RMSProp</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adadelta"><span class="toc-number">2.5.</span> <span class="toc-text">Adadelta</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adaptive-Moment-Estimation"><span class="toc-number">2.6.</span> <span class="toc-text">Adaptive Moment Estimation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AMSGrad"><span class="toc-number">2.7.</span> <span class="toc-text">AMSGrad</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Some-Conclusions"><span class="toc-number">3.</span> <span class="toc-text">Some Conclusions</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Update-Equations"><span class="toc-number">3.1.</span> <span class="toc-text">Update Equations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Visualization-of-Algorithms"><span class="toc-number">3.2.</span> <span class="toc-text">Visualization of Algorithms</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Which-algorithm-to-use"><span class="toc-number">3.3.</span> <span class="toc-text">Which algorithm to use</span></a></li></ol></li></ol>
    </div>
    
    <div class="back-top iconfont-archer">&#xe639;</div>
    <div class="sidebar sidebar-hide">
    <ul class="sidebar-tabs sidebar-tabs-active-0">
        <li class="sidebar-tab-archives"><span class="iconfont-archer">&#xe67d;</span><span class="tab-name">Archive</span></li>
        <li class="sidebar-tab-tags"><span class="iconfont-archer">&#xe61b;</span><span class="tab-name">Tag</span></li>
        <li class="sidebar-tab-categories"><span class="iconfont-archer">&#xe666;</span><span class="tab-name">Cate</span></li>
    </ul>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-panel-archives">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-and-search">
        <div class="total-archive">
        Total : 8
        </div>
        <!-- search  -->
        
    </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2019 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/14</span><a class="archive-post-title" href="/2019/04/14/K-Means/">K-Means Cluster | Python Implement</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/12</span><a class="archive-post-title" href="/2019/04/12/Optimizer/">Optimizers</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/12</span><a class="archive-post-title" href="/2019/04/12/ResNet&DenseNet/">ResNet & DenseNet</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/02</span><a class="archive-post-title" href="/2019/04/02/Regularization/">Regularization</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/27</span><a class="archive-post-title" href="/2019/03/27/碎碎念/">一些想法</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/25</span><a class="archive-post-title" href="/2019/03/25/Summary-1/">Shallow Neural Network</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/16</span><a class="archive-post-title" href="/2019/03/16/YOLO&SSD/">YOLO & SSD Algorithms</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/10</span><a class="archive-post-title" href="/2019/03/10/1/">The First Blog to TEST</a>
        </li>
    
    </ul></div>
  </div>
        <div class="sidebar-panel-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name" data-tags="K-Means"><span class="iconfont-archer">&#xe606;</span>K-Means</span>
    
        <span class="sidebar-tag-name" data-tags="Unsupervised Learning"><span class="iconfont-archer">&#xe606;</span>Unsupervised Learning</span>
    
        <span class="sidebar-tag-name" data-tags="Cluster"><span class="iconfont-archer">&#xe606;</span>Cluster</span>
    
        <span class="sidebar-tag-name" data-tags="L1 and L2 norm"><span class="iconfont-archer">&#xe606;</span>L1 and L2 norm</span>
    
        <span class="sidebar-tag-name" data-tags="Dropout"><span class="iconfont-archer">&#xe606;</span>Dropout</span>
    
        <span class="sidebar-tag-name" data-tags="Logit Regression"><span class="iconfont-archer">&#xe606;</span>Logit Regression</span>
    
        <span class="sidebar-tag-name" data-tags="Sigmoid"><span class="iconfont-archer">&#xe606;</span>Sigmoid</span>
    
        <span class="sidebar-tag-name" data-tags="cross-entropy"><span class="iconfont-archer">&#xe606;</span>cross-entropy</span>
    
        <span class="sidebar-tag-name" data-tags="ShortCut"><span class="iconfont-archer">&#xe606;</span>ShortCut</span>
    
        <span class="sidebar-tag-name" data-tags="YOLO"><span class="iconfont-archer">&#xe606;</span>YOLO</span>
    
        <span class="sidebar-tag-name" data-tags="SSD"><span class="iconfont-archer">&#xe606;</span>SSD</span>
    
        <span class="sidebar-tag-name" data-tags="DarkNet"><span class="iconfont-archer">&#xe606;</span>DarkNet</span>
    
        <span class="sidebar-tag-name" data-tags="mini-Batch"><span class="iconfont-archer">&#xe606;</span>mini-Batch</span>
    
        <span class="sidebar-tag-name" data-tags="SGD"><span class="iconfont-archer">&#xe606;</span>SGD</span>
    
        <span class="sidebar-tag-name" data-tags="Adam"><span class="iconfont-archer">&#xe606;</span>Adam</span>
    
        <span class="sidebar-tag-name" data-tags="Adagrad"><span class="iconfont-archer">&#xe606;</span>Adagrad</span>
    
        <span class="sidebar-tag-name" data-tags="Momentum"><span class="iconfont-archer">&#xe606;</span>Momentum</span>
    
    </div>
    <div class="iconfont-archer sidebar-tags-empty">&#xe678;</div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br>
    1、请确保node版本大于6.2<br>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: true
    tags: true</pre>
    </div> 
    <div class="sidebar-tags-list"></div>
</div>
        <div class="sidebar-panel-categories">
    <div class="sidebar-categories-name">
    
        <span class="sidebar-category-name" data-categories="NeuralNetworks"><span class="iconfont-archer">&#xe60a;</span>NeuralNetworks</span>
    
        <span class="sidebar-category-name" data-categories="Essay"><span class="iconfont-archer">&#xe60a;</span>Essay</span>
    
    </div>
    <div class="iconfont-archer sidebar-categories-empty">&#xe678;</div>
    <div class="sidebar-categories-list"></div>
</div>
    </div>
</div> 
    <script>
    var siteMeta = {
        root: "/",
        author: "M&"
    }
</script>
    <!-- CDN failover -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
    <script type="text/javascript">
        if (typeof window.$ === 'undefined')
        {
            console.warn('jquery load from jsdelivr failed, will load local script')
            document.write('<script src="/lib/jquery.min.js">\x3C/script>')
        }
    </script>
    <script src="/scripts/main.js"></script>
    <!-- algolia -->
    
    <!-- busuanzi  -->
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    <!-- CNZZ  -->
    
    </div>
    <!-- async load share.js -->
    
        <script src="/scripts/share.js" async></script>    
     
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>


