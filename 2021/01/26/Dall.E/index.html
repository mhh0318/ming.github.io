<!DOCTYPE html>
<html lang="en">
    <!-- title -->




<!-- keywords -->




<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" >
    <meta name="author" content="M&">
    <meta name="renderer" content="webkit">
    <meta name="copyright" content="M&">
    
    <meta name="keywords" content="明|MING's Studio,M&">
    
    <meta name="description" content="">
    <meta http-equiv="Cache-control" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
    <title>DALL·E · MingH&#39;s Studio</title>
    <style type="text/css">
    @font-face {
        font-family: 'Oswald-Regular';
        src: url("/font/Oswald-Regular.ttf");
    }

    body {
        margin: 0;
    }

    header,
    footer,
    .back-top,
    .sidebar,
    .container,
    .site-intro-meta,
    .toc-wrapper {
        display: none;
    }

    .site-intro {
        position: relative;
        z-index: 3;
        width: 100%;
        /* height: 50vh; */
        overflow: hidden;
    }

    .site-intro-placeholder {
        position: absolute;
        z-index: -2;
        top: 0;
        left: 0;
        width: calc(100% + 300px);
        height: 100%;
        background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
        background-position: center center;
        transform: translate3d(-226px, 0, 0);
        animation: gradient-move 2.5s ease-out 0s infinite;
    }

    @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }

</style>

    <link rel="preload" href= "/css/style.css?v=20180824" as="style" onload="this.onload=null;this.rel='stylesheet'" />
    <link rel="stylesheet" href= "/css/mobile.css?v=20180824" media="(max-width: 980px)">
    
    <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'" />
    
    <!-- /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/ -->
<script>
(function( w ){
	"use strict";
	// rel=preload support test
	if( !w.loadCSS ){
		w.loadCSS = function(){};
	}
	// define on the loadCSS obj
	var rp = loadCSS.relpreload = {};
	// rel=preload feature support test
	// runs once and returns a function for compat purposes
	rp.support = (function(){
		var ret;
		try {
			ret = w.document.createElement( "link" ).relList.supports( "preload" );
		} catch (e) {
			ret = false;
		}
		return function(){
			return ret;
		};
	})();

	// if preload isn't supported, get an asynchronous load by using a non-matching media attribute
	// then change that media back to its intended value on load
	rp.bindMediaToggle = function( link ){
		// remember existing media attr for ultimate state, or default to 'all'
		var finalMedia = link.media || "all";

		function enableStylesheet(){
			link.media = finalMedia;
		}

		// bind load handlers to enable media
		if( link.addEventListener ){
			link.addEventListener( "load", enableStylesheet );
		} else if( link.attachEvent ){
			link.attachEvent( "onload", enableStylesheet );
		}

		// Set rel and non-applicable media type to start an async request
		// note: timeout allows this to happen async to let rendering continue in IE
		setTimeout(function(){
			link.rel = "stylesheet";
			link.media = "only x";
		});
		// also enable media after 3 seconds,
		// which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
		setTimeout( enableStylesheet, 3000 );
	};

	// loop through link elements in DOM
	rp.poly = function(){
		// double check this to prevent external calls from running
		if( rp.support() ){
			return;
		}
		var links = w.document.getElementsByTagName( "link" );
		for( var i = 0; i < links.length; i++ ){
			var link = links[ i ];
			// qualify links to those with rel=preload and as=style attrs
			if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
				// prevent rerunning on link
				link.setAttribute( "data-loadcss", true );
				// bind listeners to toggle media back
				rp.bindMediaToggle( link );
			}
		}
	};

	// if unsupported, run the polyfill
	if( !rp.support() ){
		// run once at least
		rp.poly();

		// rerun poly on an interval until onload
		var run = w.setInterval( rp.poly, 500 );
		if( w.addEventListener ){
			w.addEventListener( "load", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		} else if( w.attachEvent ){
			w.attachEvent( "onload", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		}
	}


	// commonjs
	if( typeof exports !== "undefined" ){
		exports.loadCSS = loadCSS;
	}
	else {
		w.loadCSS = loadCSS;
	}
}( typeof global !== "undefined" ? global : this ) );
</script>

    <link rel="icon" href= "/assets/favicon.ico" />
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js" as="script" />
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" as="script" />
    <link rel="preload" href="/scripts/main.js" as="script" />
    <link rel="preload" as="font" href="/font/Oswald-Regular.ttf" crossorigin>
    <link rel="preload" as="font" href="https://at.alicdn.com/t/font_327081_1dta1rlogw17zaor.woff" crossorigin>
    
    <!-- fancybox -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script>
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  -->
    
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.3.0"></head>

    
        <body class="post-body">
    
    
<header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/" > HomePage </a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">DALL·E</a>
            </div>
    </div>
    
    <a class="home-link" href=/> Home Page </a>
</header>
    <div class="wrapper">
        <div class="site-intro" style="







height:50vh;
">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-img" style="background-image: url(https://source.unsplash.com/2560x863)"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            DALL·E
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <div class="post-intros">
                <!-- 文章页标签  -->
                
                
                    <div class="post-intro-read">
                        <span>Word count: <span class="post-count word-count">1.5k</span>Reading time: <span class="post-count reading-time">9 min</span></span>
                    </div>
                
                <div class="post-intro-meta">
                    <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                    <span class="post-intro-time">2021/01/26</span>
                    
                    <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                        <span class="iconfont-archer">&#xe602;</span>
                        <span id="busuanzi_value_page_pv"></span>
                    </span>
                    
                    <span class="shareWrapper">
                        <span class="iconfont-archer shareIcon">&#xe71d;</span>
                        <span class="shareText">Share</span>
                        <ul class="shareList">
                            <li class="iconfont-archer share-qr" data-type="qr">&#xe75b;
                                <div class="share-qrcode"></div>
                            </li>
                            <li class="iconfont-archer" data-type="weibo">&#xe619;</li>
                            <li class="iconfont-archer" data-type="qzone">&#xe62e;</li>
                            <li class="iconfont-archer" data-type="twitter">&#xe634;</li>
                            <li class="iconfont-archer" data-type="facebook">&#xe67a;</li>
                        </ul>
                    </span>
                </div>
            </div>
        
    </div>
</div>
        <script>
 
  // get user agent
  var browser = {
    versions: function () {
      var u = window.navigator.userAgent;
      return {
        userAgent: u,
        trident: u.indexOf('Trident') > -1, //IE内核
        presto: u.indexOf('Presto') > -1, //opera内核
        webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
        gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
        mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
        ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
        android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
        iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
        iPad: u.indexOf('iPad') > -1, //是否为iPad
        webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
        weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
        uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
      };
    }()
  }
  console.log("userAgent:" + browser.versions.userAgent);

  // callback
  function fontLoaded() {
    console.log('font loaded');
    if (document.getElementsByClassName('site-intro-meta')) {
      document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
      document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
      var postIntros = document.getElementsByClassName('post-intros')[0]
      if (postIntros) {
        postIntros.classList.add('post-fade-in');
      }
    }
  }

  // UC不支持跨域，所以直接显示
  function asyncCb(){
    if (browser.versions.uc) {
      console.log("UCBrowser");
      fontLoaded();
    } else {
      WebFont.load({
        custom: {
          families: ['Oswald-Regular']
        },
        loading: function () {  //所有字体开始加载
          // console.log('loading');
        },
        active: function () {  //所有字体已渲染
          fontLoaded();
        },
        inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
          console.log('inactive: timeout');
          fontLoaded();
        },
        timeout: 5000 // Set the timeout to two seconds
      });
    }
  }

  function asyncErr(){
    console.warn('script load from CDN failed, will load local script')
  }

  // load webfont-loader async, and add callback function
  function async(u, cb, err) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (cb) { o.addEventListener('load', function (e) { cb(null, e); }, false); }
    if (err) { o.addEventListener('error', function (e) { err(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }

  var asyncLoadWithFallBack = function(arr, success, reject) {
      var currReject = function(){
        reject()
        arr.shift()
        if(arr.length)
          async(arr[0], success, currReject)
        }

      async(arr[0], success, currReject)
  }

  asyncLoadWithFallBack([
    "https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js", 
    "https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js",
    "/lib/webfontloader.min.js"
  ], asyncCb, asyncErr)
</script>        
        <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <h1 id="DALL-E"><a href="#DALL-E" class="headerlink" title="DALL.E"></a>DALL.E</h1><p>Basic Understand: DALL.E = GPT-3 + VQ-VAE-2, which has 12 billion paras, compared with GPT-3 (175 billion), GPT-2(1.5 B)</p>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>DALL.E encode text(256) and image(1024) up to 1280 tokens in total, and model them <strong><em>autoregressively</em></strong>. Using maximum likelihood to trained and generated the token. This training process makes the model not only be able to generate image from scratch, but also extend the existing image. </p>
<p>Attention Mask is at each of 64 self-attention layers, which allows each image token to attend to all text tokens. Standard causal mask and sparse attention mask for image (row, column or conv-features).  The final result is reranked by CLIP, select top 32 of 512 for each caption</p>
<ul>
<li><p>One token means a symbol from a discrete vocabulary. </p>
<ul>
<li>Each <strong><em>image caption</em></strong> (or text) is represented by a 256 Byte-pair Encodings (BPE) [1] tokens with a vocabulary size of 16384 ($2^{14}$)</li>
<li>Each image is represented using 1024 tokens with a vocabulary size of 8192 ($2^{13}$)</li>
<li>BPE is a sub-word model, which is based on greedy algorithm. The size of vocabulary is a hyperparameter, which is equal to the base vocabulary size + the number of merges.  <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/gpt.html">GPT</a> has a vocabulary size of 40,478 since they have 478 base characters and chose to stop training after 40,000 merges.  <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/gpt.html">GPT-2</a> has a vocabulary size of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned with 50,000 merges.</li>
</ul>
</li>
<li><p>The images are pre-processed to 256*256, and compressed to a 32*32 grid of discrete latent variables using a discrete VAE (VQ-VAE)</p>
</li>
<li><p><strong><em>Note</em></strong> </p>
<blockquote>
<p>The discrete VAE is pre-trained by a continuous relaxation, which can obviates the need for an <strong><em>explicit codebook</em></strong>, EMA loss, or tricks like dead code revival.</p>
</blockquote>
<p>Q: Original VQ-VAE requires a codebook. DALL.E also seems need an explicit vocabulary. </p>
</li>
</ul>
<p>Flowchart of DALL.E<br>$$<br>Image\ caption\ and/or\ Image =&gt; Tokens =&gt; VQ-VAE (Decoder) =&gt; Image<br>$$</p>
<h2 id="VQ-VAE"><a href="#VQ-VAE" class="headerlink" title="VQ-VAE"></a>VQ-VAE</h2><h3 id="VQ-VAE-1"><a href="#VQ-VAE-1" class="headerlink" title="VQ-VAE-1"></a>VQ-VAE-1</h3><p><img src="/images/VQ-VAE.png" alt="VQ-VAE"></p>
<p>VQ-VAE main idea:  The dimensions are first reduced and then the encoded vectors are modelled using autoregression model like PixelCNN.</p>
<p>First we train a codebook $C[K,I]$ where $C[k,I]$ is the center vector of cluster $k$, the dimension is $K\times D$. The clusters are generated by K-Means.<br>$$<br>\begin{aligned}<br>L[X,Y] &amp;= Encoder_{\phi}(s) \\<br>z[x,y] &amp;= argmin\ ||L[x,y] - C[k]|| \\<br>\hat{L}[X,Y] &amp;= C[z[x,y]] \\<br>\hat{s} &amp;= Decoder_{\phi}(\hat{L}[X,Y])\\<br>\end{aligned}<br>$$<br>$s$ is the image, $L$ means the feature map of $L$-th layer,  $z[x,y]$ is the latent vector, while $Z[X,Y]$ is the symbolic image with bit length $XY\log_2K$</p>
<p>There are two term in the loss function, one is quantization loss, which used to align the vector space and another is reconstruction loss.  The quantization loss have two parts: <em>codebook loss</em> and <em>commitment loss</em>. Commitment loss only train encoder and codebook loss only train the codebook. Thus the whole objective function can be describe like:<br>$$<br>Loss(s,Decoder(C[k])) = ||s-Decoder(C[k])||_2^2 + ||sg[Encoder(s)] - C[k]||_2^2 + \beta||sg[C[k]]-Encoder(s)||^2_2<br>$$</p>
<p>Because the code book loss is only used for updating the codebook, thus Exponential moving average (EMA) loss can replace the codebook loss, which is the 2nd term in above equation. </p>
<p>Proof:<br>$$<br>\begin{align}<br>Loss &amp;= ||sg[E(s)] - C[k]||_2^2 \\<br>&amp;= \sum_j^K||E(s_{j})-C[j]||_2^2 \\<br>C[k] &amp;= \frac{1}{K} \sum_j^K E(s_j) \\<br>\end{align}<br>$$<br>As Minibatch, EMA is used<br>$$<br>\begin{align}<br>K^{(t)} &amp;= K^{(t-1)}\times\gamma+k^{(t)}\times(1-\gamma) \\<br>m^{(t)} &amp;= m^{(t-1)}\times\gamma+\sum_jC[j]^{(t)}(1-\gamma) \\<br>C[k] &amp;= \frac{m^{(t)}}{K^{(t)}}  \\<br>\end{align}<br>$$<br>The prior of latent variable is categorical distribution, and <em>can be modelled by the other latent variable autoregressively</em>. In training process, the Cat keeps uniform and constant. After training, fitting an autoregressive distribution over z, p(z), so that generating x via ancestral sampling(向前采样). Using PixelCNN to construct the new image.</p>
<h3 id="VQ-VAE-2"><a href="#VQ-VAE-2" class="headerlink" title="VQ-VAE-2"></a>VQ-VAE-2</h3><p><img src="/images/VQ-VAE-2.png" alt="VQ-VAE-2"></p>
<p>Compared with VQ-VAE, the new version has several tricks to improve the performance.</p>
<h4 id="Hierarchical-Latent-Codes"><a href="#Hierarchical-Latent-Codes" class="headerlink" title="Hierarchical Latent Codes"></a>Hierarchical Latent Codes</h4><p>The input to the model is a 256×256 image that is compressed to quantized latent maps of size 64×64 and 32×32 for the bottom and top levels, respectively.  Top latent codes model global information, while bottom latent (conditional top code) code model local details.  The encoder network first transforms and downsamples the image by a factor of 4 to a 64×64 representation which is quantized to our bottom level latent map.  Another stack of residual blocks then further scales down the representations by a factor of 2, yielding a top-level 32×32 latent map after quantization. Decoder consists of a few residual blocks followed by a number of strided transposed convolutions to upsample the representations back to the original image size.</p>
<h4 id="Multi-headed-Self-attention"><a href="#Multi-headed-Self-attention" class="headerlink" title="Multi-headed Self-attention"></a>Multi-headed Self-attention</h4><blockquote>
<p>Fitting prior distributions using neural networks from training data has become common practice, as it can significantly improve the performance of latent variable models. This procedure also reduces the gap between the marginal posterior and the prior. Thus, latent variables sampled from the learned prior at test time are close to what the decoder network has observed during training which results in more coherent outputs. From an information theoretic point of view, the process of fitting a prior to the learned posterior can be considered as loss less compression of the latent space by re-encoding the latent variables with a distribution that is a better approximation of their true distribution, and thus results in bit rates closer to Shannon’s entropy. Therefore the lower the gap between the true entropy and the negative log-likelihood of the learned prior, the more realistic image samples one can expect from decoding the latent samples.</p>
</blockquote>
<p>The prior over the top latent map is responsible for structural global information. Thus, we equip it with multi-headed self-attention layers as in [2] so it can benefit from a larger receptive field to capture correlations in spatial locations that are far apart in the image. </p>
<p>Using self-attention layers in PixelCNN in the bottom-level prior would not be practical due to memory constraints.</p>
<p>The residual gated convolution layers of PixelCNN are interspersed with causal multi-headed attention every five layers.</p>
<h4 id="Rejection-Sampling"><a href="#Rejection-Sampling" class="headerlink" title="Rejection Sampling"></a>Rejection Sampling</h4><p>probabilistic models trained with the maximum likelihood objective are forced to model all of the training data distribution.</p>
<blockquote>
<p>This is because the MLE objective can be expressed as the forward KL-divergence between the data and model distributions, which would be driven to infinity if an example in the training data is assigned zero mass.    </p>
</blockquote>
<p>Ancestral sampling from autoregressive models can in practice induce errors that can accumulate over long sequences and result in samples with reduced quality.</p>
<p>Use a classifier network that is trained on ImageNet to score samples from our model according to the probability the classifier assigns to the correct class.</p>
<h2 id="Continuous-Relaxation"><a href="#Continuous-Relaxation" class="headerlink" title="Continuous Relaxation"></a>Continuous Relaxation</h2><p>The VQ-VAE in DALL.E is trained with continuous relaxation. As mentioned in the blog, there are two reference about continuous relaxation, The Concrete Distribution and Gumbel Softmax, namely. </p>
<h3 id="Reparameterization-Trick"><a href="#Reparameterization-Trick" class="headerlink" title="Reparameterization Trick"></a>Reparameterization Trick</h3><p><img src="/images/repara.png" alt="repara"></p>
<p>Let $Z∼N(μ(X),σ2(X))$. The parameters of the Gaussian are a function of the input $X$ - e.g. the output of stacked dense layers. When sampling realizations of $Z$, gradients won’t be able to propagate to the weights of the dense layers. We can substitute $Z$ with a different random variable $Z′=μ(X)+σ(X)⋅E$ where $E∼N(0,1)$. Now the sampling will be from $E$, so the gradients won’t propagate through this path - which we don’t care about. However, through $μ(X)$ and $σ(X)$ they will, since it’s a deterministic path.</p>
<h3 id="Gumbel-Softmax-Concrete-Distribution"><a href="#Gumbel-Softmax-Concrete-Distribution" class="headerlink" title="Gumbel-Softmax / Concrete Distribution"></a>Gumbel-Softmax / Concrete Distribution</h3><p><img src="/images/GumbelSoftmax.png" alt="gsd"></p>
<p>Gumbel-Max Trick<br>$$<br>\begin{align}<br>z &amp;= argmax[g_i+\log (\pi_i)] \\<br>g_i &amp; \sim Gumbel(0,1)\\<br>Gumbel(0,1) &amp;= -\log(-\log(u)) \\<br>u &amp;\sim Uniform(0,1)\\<br>Gumbel:F(x) &amp;= e^{-e^{-x}}<br>\end{align}<br>$$<br>Argmax means the Gumbel max is not continuous, thus we can covert the $Z$ to one-hot formula and using softmax method.<br>$$<br>\begin{align}<br>y_i&amp;=\frac{\exp(z_k/τ)}{∑_{k=1}^K \exp(z_k/τ)}\\<br>&amp;=\frac{\exp((g_i+\log (\pi_i))/\tau)}{∑_{k=1}^K \exp((g_k+\log (\pi_k))/\tau)}<br>\end{align}<br>$$</p>
<h2 id="GPT-3"><a href="#GPT-3" class="headerlink" title="GPT-3"></a>GPT-3</h2><p>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.07909">Sennrich, Rico, Barry Haddow, and Alexandra Birch. “Neural machine translation of rare words with subword units.” <em>arXiv preprint arXiv:1508.07909</em> (2015).</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.05751">Parmar, Niki, et al. “Image transformer.” <em>International Conference on Machine Learning</em>. PMLR, 2018.</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.00937">Oord, Aaron van den, Oriol Vinyals, and Koray Kavukcuoglu. “Neural discrete representation learning.” <em>arXiv preprint arXiv:1711.00937</em> (2017).</a></p>
<p>[4] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.00446">Razavi, Ali, Aaron van den Oord, and Oriol Vinyals. “Generating diverse high-fidelity images with vq-vae-2.” <em>arXiv preprint arXiv:1906.00446</em> (2019).</a></p>

    </article>
    <!-- license  -->
    
    <!-- paginator  -->
    <ul class="post-paginator">
        <li class="next">
            
        </li>
        <li class="previous">
            
                <div class="prevSlogan">Previous Post</div>
                <a href= "/2019/04/12/Optimizer/" title= "Optimizers">
                    <div class="prevTitle">Optimizers</div>
                </a>
            
        </li>
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    
    <!-- partial('_partial/comment/changyan') -->
    <!--PC版-->


    
    
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
    <div id="comment"></div>
    <script>
    new Valine({
        el: '#comment' ,
        notify:false, 
        verify:false, 
        appId: "NSjyzLH1Tpagwt9WJy9mTjak-gzGzoHsz",
        appKey: "ud7gLUDDmxtfrI0rUEboicQe",
        placeholder: "Do not be shy to chat here, guys.",
        path:window.location.pathname, 
        avatar:'mm' 
    });
    </script>


    <!-- 评论 -->
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:mercihu0318@gmail.com" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/mhh0318" class="iconfont-archer github" target="_blank" title=github></a>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">A Rookie in <a target="_blank"> Machine Learning </a>
    </div>
    <!-- 不蒜子  -->
    
    <div class="busuanzi-container">
    
     
    <span id="busuanzi_container_site_pv">PV : <span id="busuanzi_value_site_pv"></span></span>
    
    </div>
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper" style=
    







top:50vh;

    >
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#DALL-E"><span class="toc-number">1.</span> <span class="toc-text">DALL.E</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Overview"><span class="toc-number">1.1.</span> <span class="toc-text">Overview</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VQ-VAE"><span class="toc-number">1.2.</span> <span class="toc-text">VQ-VAE</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#VQ-VAE-1"><span class="toc-number">1.2.1.</span> <span class="toc-text">VQ-VAE-1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VQ-VAE-2"><span class="toc-number">1.2.2.</span> <span class="toc-text">VQ-VAE-2</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Hierarchical-Latent-Codes"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">Hierarchical Latent Codes</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Multi-headed-Self-attention"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">Multi-headed Self-attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Rejection-Sampling"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">Rejection Sampling</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Continuous-Relaxation"><span class="toc-number">1.3.</span> <span class="toc-text">Continuous Relaxation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Reparameterization-Trick"><span class="toc-number">1.3.1.</span> <span class="toc-text">Reparameterization Trick</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gumbel-Softmax-Concrete-Distribution"><span class="toc-number">1.3.2.</span> <span class="toc-text">Gumbel-Softmax &#x2F; Concrete Distribution</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GPT-3"><span class="toc-number">1.4.</span> <span class="toc-text">GPT-3</span></a></li></ol></li></ol>
    </div>
    
    <div class="back-top iconfont-archer">&#xe639;</div>
    <div class="sidebar sidebar-hide">
    <ul class="sidebar-tabs sidebar-tabs-active-0">
        <li class="sidebar-tab-archives"><span class="iconfont-archer">&#xe67d;</span><span class="tab-name">Archive</span></li>
        <li class="sidebar-tab-tags"><span class="iconfont-archer">&#xe61b;</span><span class="tab-name">Tag</span></li>
        <li class="sidebar-tab-categories"><span class="iconfont-archer">&#xe666;</span><span class="tab-name">Cate</span></li>
    </ul>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-panel-archives">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-and-search">
        <div class="total-archive">
        Total : 5
        </div>
        <!-- search  -->
        
    </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2021 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/26</span><a class="archive-post-title" href= "/2021/01/26/Dall.E/" >DALL·E</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2019 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/12</span><a class="archive-post-title" href= "/2019/04/12/Optimizer/" >Optimizers</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/02</span><a class="archive-post-title" href= "/2019/04/02/Regularization/" >Regularization</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/25</span><a class="archive-post-title" href= "/2019/03/25/Summary-1/" >Shallow Neural Network</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/10</span><a class="archive-post-title" href= "/2019/03/10/1/" >The First Blog to TEST</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-panel-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name" data-tags="L1 and L2 norm"><span class="iconfont-archer">&#xe606;</span>L1 and L2 norm</span>
    
        <span class="sidebar-tag-name" data-tags="Dropout"><span class="iconfont-archer">&#xe606;</span>Dropout</span>
    
        <span class="sidebar-tag-name" data-tags="mini-Batch"><span class="iconfont-archer">&#xe606;</span>mini-Batch</span>
    
        <span class="sidebar-tag-name" data-tags="SGD"><span class="iconfont-archer">&#xe606;</span>SGD</span>
    
        <span class="sidebar-tag-name" data-tags="Adam"><span class="iconfont-archer">&#xe606;</span>Adam</span>
    
        <span class="sidebar-tag-name" data-tags="Adagrad"><span class="iconfont-archer">&#xe606;</span>Adagrad</span>
    
        <span class="sidebar-tag-name" data-tags="Momentum"><span class="iconfont-archer">&#xe606;</span>Momentum</span>
    
        <span class="sidebar-tag-name" data-tags="Logit Regression"><span class="iconfont-archer">&#xe606;</span>Logit Regression</span>
    
        <span class="sidebar-tag-name" data-tags="Sigmoid"><span class="iconfont-archer">&#xe606;</span>Sigmoid</span>
    
        <span class="sidebar-tag-name" data-tags="cross-entropy"><span class="iconfont-archer">&#xe606;</span>cross-entropy</span>
    
    </div>
    <div class="iconfont-archer sidebar-tags-empty">&#xe678;</div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: true
    tags: true</pre>
    </div> 
    <div class="sidebar-tags-list"></div>
</div>
        <div class="sidebar-panel-categories">
    <div class="sidebar-categories-name">
    
        <span class="sidebar-category-name" data-categories="Essay"><span class="iconfont-archer">&#xe60a;</span>Essay</span>
    
        <span class="sidebar-category-name" data-categories="NeuralNetworks"><span class="iconfont-archer">&#xe60a;</span>NeuralNetworks</span>
    
    </div>
    <div class="iconfont-archer sidebar-categories-empty">&#xe678;</div>
    <div class="sidebar-categories-list"></div>
</div>
    </div>
</div> 
    <script>
    var siteMeta = {
        root: "/",
        author: "M&"
    }
</script>
    <!-- CDN failover -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
    <script type="text/javascript">
        if (typeof window.$ === 'undefined')
        {
            console.warn('jquery load from jsdelivr failed, will load local script')
            document.write('<script src="/lib/jquery.min.js">\x3C/script>')
        }
    </script>
    <script src="/scripts/main.js"></script>
    <!-- algolia -->
    
    <!-- busuanzi  -->
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    <!-- CNZZ  -->
    
    </div>
    <!-- async load share.js -->
    
        <script src="/scripts/share.js" async></script>    
     
    </body>
</html>


